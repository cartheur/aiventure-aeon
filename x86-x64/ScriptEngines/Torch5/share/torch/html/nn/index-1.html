<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN" "http://www.w3.org/TR/html4/strict.dtd">
<html>
<head>
<meta http-equiv="Content-Type" content="text/html; charset=iso-8859-1">
<title>

 Detailed Overview of the Neural Network Package
</title>
<link href="doctorch.css" rel="stylesheet" type="text/css">
</head>

<body class="torchdoc">
<div id="container">

<div id="header">
<h3>
 Detailed Overview of the Neural Network Package</h3>
</div>

<div id="navline">
<a href="../index.html">Torch Manual</a>&nbsp; > &nbsp; <a href="index.html">
Neural Network Package</a>&nbsp; > &nbsp; <b>
 Detailed Overview of the Neural Network Package</b>
</div>

<div id="contents">

<p>

<b><code>Module</code></b>
<p>

A neural network is called a <a href="index-2-1.html#Module"><code>Module</code></a> (or simply
<i>module</i> in this documentation) in Torch. <code>Module</code> is an abstract
class which defines four main methods:
<ul><li><a href="index-2-1-1.html#ModuleForward"><code>forward(input)</code></a> which computes the output
of the module given the <code>input</code> <a href="../torch/index-6.html#Tensor"><code>Tensor</code></a>.
</li><li><a href="index-2-1-2.html#ModuleBackward"><code>backward(input, gradOutput)</code></a> which computes the gradients of the module
with respect to its own parameters, and its own inputs.
</li><li><a href="index-2-1-3.html#ModuleZeroGradParameters"><code>zeroGradParameters()</code></a> which zeroes the gradient with respect to the parameters of
the module.
</li><li><a href="index-2-1-4.html#ModuleUpdateParameters"><code>updateParameters(learningRate)</code></a> which updates the parameters after one has computed
the gradients with <code>backward()</code>
</li></ul><p>

It also declares two members:
<ul><li><a href="index-2-1-7-1.html#ModuleOutput"><code>output</code></a> which is the output returned by <code>forward()</code>.
</li><li><a href="index-2-1-7-2.html#ModuleGradInput"><code>gradInput</code></a> which contains the gradients with respect to the input of the module,
computed in a <code>backward()</code>.
</li></ul><p>

Two other perhaps less used but handy methods are also defined:
<ul><li><a href="index-2-1-5.html#ModuleShare"><code>share(mlp,s1,s2,...,sn)</code></a> which makes this module share the parameters s1,..sn of the module <code>mlp</code>. This is useful if you want to have modules that share the same weights.
</li><li><a href="index-2-1-6.html#ModuleClone"><code>clone(...)</code></a> which produces a deep copy of (i.e. not just a pointer to) this Module, including the current state of its parameters (if any). 
</li></ul><p>

Some important remarks:
<ul><li><code>output</code> contains only valid values after a
<a href="index-2-1-1.html#ModuleForward"><code>forward(input)</code></a>.
</li><li><code>gradInput</code> contains only
valid values after a <a href="index-2-1-2.html#ModuleBackward"><code>backward(input, gradOutput)</code></a>.
</li><li><a href="index-2-1-2.html#ModuleBackward"><code>backward(input, gradOutput)</code></a> uses certain computations obtained
during <a href="index-2-1-1.html#ModuleForward"><code>forward(input)</code></a>. You <i>must</i> call <code>forward()</code> before
calling a <code>backward()</code>, on the <i>same</i> <code>input</code>, or your gradients are going to be incorrect!
</li></ul><p>

<p>

<b>Plug and play</b>
<p>

Building a simple neural network can be achieved by constructing an available layer.
A linear neural network (perceptron!) is built only in one line:
<pre>
nn = nn.Linear(10,1) -- perceptron with 10 inputs
</pre>
<p>

More complex neural networks are easily built using container classes
<a href="index-2-2-2.html#Sequential"><code>Sequential</code></a> and <a href="index-2-2-1.html#Concat"><code>Concat</code></a>. <code>Sequential</code> plugs
layer in a feed-forward fully connected manner. <code>Concat</code> concatenates in
one layer several modules: they take the same inputs, and their output is
concatenated.
<p>

Creating a one hidden-layer multi-layer perceptron is thus just as easy as:
<pre>
mlp = nn.Sequential()
mlp:add( nn.Linear(10, 25) ) -- 10 input, 25 hidden units
mlp:add( nn.Tanh() ) -- some hyperbolic tangent transfer function
mlp:add( nn.Linear(25, 1) ) -- 1 output
</pre>
<p>

Of course, <code>Sequential</code> and <code>Concat</code> can contains other <code>Sequential</code> or
<code>Concat</code>, allowing you to try the craziest neural networks you ever dreamt
of! See the <a href="index-2.html#Modules">complete list of available modules</a>.
<p>

<b>Training a neural network</b>
<p>

Once you built your neural network, you have to choose a particular <a href="index-3.html#Criterions"><code>Criterion</code></a> to
train it. A criterion is a class which describes the cost to be minimized during training.
<p>

You can then train the neural network by using the <a href="index-4-1.html#StochasticGradient"><code>StochasticGradient</code></a> class.
<pre>
 criterion = nn.MSECriterion(1) -- Mean Squared Error criterion
 trainer = nn.StochasticGradient(mlp, criterion)
 trainer:train(dataset) -- train using some examples
</pre>
<p>

StochasticGradient expect as a <code>dataset</code> an object which implements the operator
<code>dataset[index]</code> and implements the method <code>dataset:size()</code>. The <code>size()</code> methods
returns the number of examples and <code>dataset[i]</code> has to return the i-th example.
<p>

An <code>example</code> has to be an object which implements the operator
<code>example[field]</code>, where <code>field</code> might take the value <code>1</code> (input features)
or <code>2</code> (corresponding label which will be given to the criterion). 
The input is usually a Tensor (except if you use special kind of gradient modules,
like <a href="index-2-6.html#TableLayers">table layers</a>). The label type depends of the criterion.
For example, the <a href="index-3-5.html#MSECriterion"><code>MSECriterion</code></a> expect a Tensor, but the
<a href="index-3-3.html#ClassNLLCriterion"><code>ClassNLLCriterion</code></a> except a integer number (the class).
<p>

Such a dataset is easily constructed by using Lua tables, but it could any <code>C</code> object
for example, as long as required operators/methods are implemented.
<a href="index-4-2.html#DoItStochasticGradient">See an example</a>.
<p>

<code>StochasticGradient</code> being written in <code>Lua</code>, it is extremely easy to
cut-and-paste it and create a variant to it adapted to your needs (if the
constraints of <code>StochasticGradient</code> do not satisfy you).
<p>

<b>Low Level Training Of a Neural Network</b>
<p>

If you want to program the <code>StochasticGradient</code> by hand, you essentially need to control the use of forwards and backwards through the network yourself.
For example, here is the code fragment one would need to make a gradient step given an input <code>x</code>, a desired output <code>y</code>, a network <code>mlp</code> and a given criterion <code>criterion</code> and learning rate <code>learningRate</code>:
<pre>
function gradUpdate(mlp, x, y, criterion, learningRate) 
  local pred = mlp:forward(x)
  local err = criterion:forward(pred, y)
  local gradCriterion = criterion:backward(pred, y)
  mlp:zeroGradParameters()
  mlp:backward(x, gradCriterion)
  mlp:updateParameters(learningRate)
end
</pre>
For example, if you wish to use your own criterion you can simple replace 
<code>gradCriterion</code> with the gradient vector of your criterion of choice.
<p>

<p>

</div>



<div id="footer-left">&nbsp;<a href=""></a></div>
<div id="footer-right"><a href="index-2.html">
 Modules</a>&nbsp;</div>

<div id="last-modified">

</div>

</div>
</body>
</html>
