<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN" "http://www.w3.org/TR/html4/strict.dtd">
<html>
<head>
<meta http-equiv="Content-Type" content="text/html; charset=iso-8859-1">
<title>

 Example of training using StochasticGradient
</title>
<link href="doctorch.css" rel="stylesheet" type="text/css">
</head>

<body class="torchdoc">
<div id="container">

<div id="header">
<h3>
 Example of training using StochasticGradient</h3>
</div>

<div id="navline">
<a href="../index.html">Torch Manual</a>&nbsp; > &nbsp; <a href="index.html">
Neural Network Package</a>&nbsp; > &nbsp; <a href="index-4.html">
 Training a neural network</a>&nbsp; > &nbsp; <b>
 Example of training using <code>StochasticGradient</code></b>
</div>

<div id="contents">
<a name="DoItStochasticGradient"></a>

<p>

<p>

We show an example here on a classical XOR problem.
<p>

<b>Dataset</b>
<p>

We first need to create a dataset, following the conventions described in
<a href="index-4-1-2.html#StochasticGradientTrain"><code>StochasticGradient</code></a>.
<pre>
require "lab"
dataset={};
function dataset:size() return 100 end -- 100 examples
for i=1,dataset:size() do 
  local input = lab.randn(2);     -- normally distributed example in 2d
  local output = torch.Tensor(1);
  if input[1]*input[2]>0 then     -- calculate label for XOR function
    output[1] = -1;
  else
    output[1] = 1
  end
  dataset[i] = {input, output}
end
</pre>
<p>

<b>Neural Network</b>
<p>

We create a simple neural network with one hidden layer.
<pre>
require "nn"
mlp = nn.Sequential();  -- make a multi-layer perceptron
inputs = 2; outputs = 1; HUs = 20; -- parameters
mlp:add(nn.Linear(inputs, HUs))
mlp:add(nn.Tanh())
mlp:add(nn.Linear(HUs, outputs))
</pre>
<p>

<b>Training</b>
<p>

We choose the Mean Squared Error criterion and train the beast.
<pre>
criterion = nn.MSECriterion()  
trainer = nn.StochasticGradient(mlp, criterion)
trainer.learningRate = 0.01
trainer:train(dataset)
</pre>
<p>

<b>Test the network</b>
<p>

<pre>
x = torch.Tensor(2)
x[1] =  0.5; x[2] =  0.5; print(mlp:forward(x))
x[1] =  0.5; x[2] = -0.5; print(mlp:forward(x))
x[1] = -0.5; x[2] =  0.5; print(mlp:forward(x))
x[1] = -0.5; x[2] = -0.5; print(mlp:forward(x))
</pre>
<p>

You should see something like:
<pre>
> x = torch.Tensor(2)
> x[1] =  0.5; x[2] =  0.5; print(mlp:forward(x))

-0.3490
[torch.Tensor of dimension 1]

> x[1] =  0.5; x[2] = -0.5; print(mlp:forward(x))

 1.0561
[torch.Tensor of dimension 1]

> x[1] = -0.5; x[2] =  0.5; print(mlp:forward(x))

 0.8640
[torch.Tensor of dimension 1]

> x[1] = -0.5; x[2] = -0.5; print(mlp:forward(x))

-0.2941
[torch.Tensor of dimension 1]
</pre>
<p>

</div>



<div id="footer-left">&nbsp;<a href="index-4-1.html">
 <code>StochasticGradient</code></a></div>
<div id="footer-right"><a href="index-4-3.html">
 Example of manual training of a neural network</a>&nbsp;</div>

<div id="last-modified">

</div>

</div>
</body>
</html>
