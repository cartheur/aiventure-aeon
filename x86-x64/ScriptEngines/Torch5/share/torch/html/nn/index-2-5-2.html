<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN" "http://www.w3.org/TR/html4/strict.dtd">
<html>
<head>
<meta http-equiv="Content-Type" content="text/html; charset=iso-8859-1">
<title>

 SpatialSubSampling
</title>
<link href="doctorch.css" rel="stylesheet" type="text/css">
</head>

<body class="torchdoc">
<div id="container">

<div id="header">
<h3>
 SpatialSubSampling</h3>
</div>

<div id="navline">
<a href="../index.html">Torch Manual</a>&nbsp; > &nbsp; <a href="index.html">
Neural Network Package</a>&nbsp; > &nbsp; <a href="index-2.html">
 Modules</a>&nbsp; > &nbsp; <a href="index-2-5.html">
 Convolutional layers</a>&nbsp; > &nbsp; <b>
 <code>SpatialSubSampling</code></b>
</div>

<div id="contents">

<p>

<pre>
module = nn.SpatialSubSampling(nInputPlane, kW, kH, [dW], [dH])
</pre>
<p>

Applies a 2D sub-sampling over an input image composed of several input planes. The <code>input</code> tensor in
<code>forward(input)</code> is expected to be a 3D tensor (<code>width x height x nInputPlane</code>). The number of output
planes will be the same as <code>nInputPlane</code>.
<p>

The parameters are the following:
<dl><dt> <code>nInputPlane</code></dt><dd>The number of expected input planes in the image given into <code>forward()</code>.
</dd><dt> <code>kW</code></dt><dd>The kernel width of the sub-sampling
</dd><dt> <code>kH</code></dt><dd>The kernel height of the sub-sampling
</dd><dt> <code>dW</code></dt><dd>The step of the sub-sampling in the width dimension. Default is <code>1</code>.
</dd><dt> <code>dH</code></dt><dd>The step of the sub-sampling in the height dimension. Default is <code>1</code>.
</dd></dl><p>

Note that depending of the size of your kernel, several (of the last)
columns or rows of the input image might be lost. It is up to the user to
add proper padding in images.
<p>

If the input image is a 3D tensor <code>width x height x nInputPlane</code>, the output image size
will be <code>owidth x oheight x nInputPlane</code> where
<pre>
owidth  = (width  - kW) / dW + 1
oheight = (height - kH) / dH + 1 .
</pre>
<p>

The parameters of the sub-sampling can be found in <code>self.weight</code> (Tensor of
size <code>nInputPlane</code>) and <code>self.bias</code> (Tensor of size <code>nInputPlane</code>). The
corresponding gradients can be found in <code>self.gradWeight</code> and
<code>self.gradBias</code>.
<p>

The output value of the layer can be precisely described as:
<pre>
output[i][j][k] = bias[k]
  + weight[k] sum_{s=1}^kW sum_{t=1}^kH input[dW*(i-1)+s)][dH*(j-1)+t][k]
</pre>
<p>

</div>



<div id="footer-left">&nbsp;<a href="index-2-5-1.html">
 <code>SpatialConvolution</code></a></div>
<div id="footer-right"><a href="index-2-5-3.html">
 <code>TemporalConvolution</code></a>&nbsp;</div>

<div id="last-modified">

</div>

</div>
</body>
</html>
